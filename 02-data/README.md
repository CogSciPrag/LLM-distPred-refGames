# Data

This directory contains results files which are the product of running the models with various methods and on dofferent tasks (for purposes of exploration and checking of the derivation methodology).

The contents of the directory are as follows.
The top level directories contain results of different tasks, where the main results for the paper are contained in `refgame` (the source file with items for this task is the `results_GPT.csv`). Files in `boolq` are results of log probability based scoring on the BoolQ benchmark (the formatted input file `super_glue_formatted_boolq.csv` was constructed from its test split within the HuggingFace superglue dataset). Files in `manual_sanity_check` present results on a small dataset `sanity_check_data.csv` which was manually constructed for checking sensibility of LLM scores. Please refer to the README in refgame for more details on the interpretation of subdirectory names and contents.

**IMPORTANT NOTE:** results from the human experiment (`data-raw-human-old.csv` and `data-raw-human.csv`) and GPT results (`results_GPT.csv`) are duplicated in each leaf subdirectories of interest (mean_scores_forward and sum_scores_forward), so that files from sudirectories can be processed as a package. The human results are identical in all their occurrences. GPT results only differ in aggregation method: the "mean_scores" directories contain GPT results scores aggregated as averages over token log probabilities (computed post hoc with the help of the `tiktoken` tokenizer for gpt-3.5-turbo); the "sum_scores" directories contain the original scores computed as sums over token log probabilities. See the single directory's READMEs for more details.
Note that not all leaf directories contain results from all models because most exploratory work was done only on a few models.
Note that the BoolQ and manual-check results have a *different format* (only a subset of the refgame columns).