---
title: "LLMs as probabilistic models for human experimental data"
author: "Michael Franke, Polina Tsvilodub and Fausto Carcassi"
format: 
  html: 
    code-fold: true
    code-summary: "Show the code"
    self-contained: true
execute:
  error: false
  warning: false
  message: false
  cache: false
callout-appearance: simple
editor:
  markdown:
    wrap: sentence
---

```{r preamble}
#| echo: false

# dependencies etc.
source("00-premable.r")

rerun = TRUE
source('00-stan-fit-helpers.R')

# which model to test
model = "probability"
```

# Motivation

Recent work comparing LLMs to human choice behavior in psychological experiments has focused on whether LLMs predict patterns of human answer behavior qualitatively.
The bulk of this work looks at the answers generated by the LLMs.
<!-- Especially when it comes to sophisticated LLMs, like the GPT-X family, this is problematic because it may conflate the core LLM behavior with potentially unkown sophisticated decoding strategies. -->
In general, much less attention has been payed to whether LLMs can also make adequate *quantitative predictions* that match human choice probabilities in suitable experiments.
(Notable exceptions at the interface of NLP and computational psycholinguistics (Linzen, Hu, Rabovsky ...).)

This work looks at whether probabilistic predictions of GTP-3.5 match human answer patterns also quantitatively.
Rather than looking at LLMs as behavior-generating black box, which we want to understand based on its input-output behvaior, we explore whether we can use LLMs as a predictive probabilistic model, or at least as a component of one, aiming to predict the full distribution of human experimental data.

# Experiment: Reference games

To keep matters simple, we use an established, well-understood and austere experimental paradigm to test human decision making in abstract communicative tasks, so-called **reference games**.
A reference game consists of two players, a speaker and an interpreter, who jointly observe a set of objects (the so-called context).
The speaker is assigned an object from the context set which they have to describe to the interpreter.
The interpreter observes the speaker's description, and chooses one of the objects from the context set.
The goal of the game is, for the speaker, to choose a description that enables the interpreter to choose the target object; and, for the listener, to guess correctly which object the speaker had in mind.

For example, Figure~<span style = "color:firebrick">XXX</span> shows an example, taken from <span style = "color:firebrick">Frank & Goodman (2012)</span>, which we refer to as the **classic reference game example**.
In this example, there are two features that differ across three objects (here shape and color).
One object shares both its color and shape with one other object, while the two other objects have one unique feature (e.g., being the only circle, or the only green object).
In a critical trial for the speaker, the target object to describe is one of the two objects with a unique feature.
... <span style = "color:firebrick">TODO: describe "classic example" </span> ...

The code for the experiment can be found [here](https://github.com/magpie-ea/magpie3-text-refgame), and a live version of the experiment can be tested [here](https://magpie-ea.github.io/magpie3-text-refgame/).

## Participants 

Three hundred participants were recruited via Prolific for monetary compensation (0.45 pounds, corresponding to 15.40 pounds / hour).

## Materials & Design

While previous reference games with humans used pictorial representations of objects, and sometimes even pictorial representations of messages, we implemented a simple reference game in a completely text-based fashion.
See Figure <span style = "color:firebrick">XXX</span> for the precise text used.
Each context consisted of three objects.
Objects are defined by a triple of properties, namely a color, a shape and a texture.
The colors were ..., the shapes were ..., the textures were ....
...

To create the stimulus material for both the human experiments and the LLM simulations, we sampled 100 reference games, all of which are instances of the "classic reference game" show in Figure <span style = "color:firebrick">XXX</span>.
The sampled reference games differed in terms of the precise objects that instantiated the "classic example", and in terms of the order in which the objects and expression alternatives were presented in the text.

## Procedure 

For each participant the experiment sampled four different instances from our list of 100 pre-created reference games.
Participants played two of these as a speaker (**production condition**), and the other two as an interpreter (**interpretation condition**).

## Results

The overall distribution of choices that correspond to the target, competitor, and distractor states is this:

```{r choice-counts-human-data}

tcd <- c("target", "competitor", "distractor")

d <- read_csv('02-data-prepped.csv') |> 
  mutate(
    response   = factor(response  , levels = tcd),
    prediction = factor(prediction, levels = tcd),
    condition  = factor(condition,  levels = c("production", "interpretation"))
    ) 

d_counts <- d |> 
  count(condition, response) |> 
  group_by(condition) |> 
  mutate(total = sum(n)) |> 
  ungroup() |> 
  mutate(proportion = n / total)
  
# d_counts |> 
#   ggplot(aes(x = response, y = n, fill = response)) +
#   geom_col(position = "dodge") +
#   facet_wrap(.~condition) +
#   ylab("") + xlab("") +
#   theme(legend.position="none")

refgame_counts <- d_counts |> 
  ggplot(aes(x = response, y = n, fill = response)) +
  geom_col(position = "dodge") +
  facet_grid(condition ~ .) +
  ylab("") + xlab("") +
  theme(legend.position="none") +
  ylim(c(0,604)) +
  ggtitle("human data") +
  theme(axis.text.x = element_text(angle = 25, vjust = 1, hjust=1)) +
  theme(strip.text = element_text(size = 12)) + 
  theme(plot.title = element_text(size = 12))
  
refgame_counts

ggsave(plot = refgame_counts, 
       filename = "../04-paper/00-pics/refgame-counts.pdf", 
       height = 4, width = 3, scale = 1.1)
  

```

It is interesting that the distractor options were chosen rather often.
We also see that the number of target choices is higher in the production condition than in the interpretation condition.
This is in line with previous experimental results ... <span style = "color:firebrick">elaborate</span> ...

The next section looks at different ways of deriving quantitative predictions from an LLM to fit to the human data.

# LLM predictions for reference games

## Probabilistic predictions from LLMs

Since the reference game descriptions and choice options are all text-based, the quantitative predictions of LLMs can easily be derived form the conditional probabilities assigned the answer options after processing the context description.
Concretely, we prompted GPT-3.5 with each context description $C$ for all of the 100 pre-generated reference games, and then recorded the probabilities $P(o_i \mid C)$ assigned to all answer options.
If an answer option $o_i = w_{i1}, \dots, w_{in}$ consisted of more than one word, like in the interpretation condition, the conditional probabilities are taken to be the total word probabilities: $P(o_i \mid C) = \frac{1}{n} \prod_{j=1}^n P(w_{ij} \mid C, w_{i(j-1)})$.
Averaging over all contexts $C_1, \dots C_m$ which occurred in the experiment (including double occurrences, but separating production and interpretation conditions, so as to produce the aggregate predictions for exactly the set of conditions that the participant group saw), the model's (non-normalized) average prediction for choice option $o_i$ is:

$$P(o_i) = \frac{1}{m} \sum_{k=1}^{m} P(o_i \mid C_k)$$

The model's **vanilla prediction** for the probability of option $o_i$ is then the normalized probability over all available options:

$$\text{Prediction}(o_i) = \frac{P(o_i)}{\sum_{o_j} P(o_j)}$$

The vanilla prediction of the LLM is shown in the Figure below.

```{r stay-in-log-space}

softmax_row <- function(matrix) {
  exp_matrix <- exp(matrix)
  row_sums <- rowSums(exp_matrix)
  softmax_matrix <- exp_matrix / row_sums
  return(softmax_matrix)
}

# production
x <- d |> 
  # use only predictions for items in the relevant condition
  filter(condition == "production") |> 
  select(submission_id, trial_nr, item, condition, starts_with("scores_produ")) |> 
  mutate(scores_production_distractor = 
           log(exp(scores_production_distractor1) + exp(scores_production_distractor2))) |> 
  select(-scores_production_distractor1, -scores_production_distractor2) |> 
  select(starts_with("scores_produ")) |> as.matrix()
  
# old predictions
LLM_pred_old_prod <- apply(softmax_row(x), 2, mean)

# new predictions
y <- apply(x, 2, mean)
LLM_pred_new_prod <- exp(y)/ sum(exp(y))

alpha = 5
(z1 <- LLM_pred_new_prod ^ alpha / sum(LLM_pred_new_prod ^ alpha))
(z2 <- exp(alpha * y)/ sum(exp(alpha * y)))
z1 - z2
z1 == z2


# interpretation
x <- d |> 
  # use only predictions for items in the relevant condition
  filter(condition == "interpretation") |> 
  select(starts_with("scores_inter")) |> as.matrix()

# old predictions
LLM_pred_old_inter <- apply(softmax_row(x), 2, mean)

# new predictions
y <- apply(x, 2, mean)
LLM_pred_new_inter <- exp(y)/ sum(exp(y))

```


```{r get-and-plot-average-llm-predictions}

d_llm_prob_prod <- d |> 
  # use only predictions for items in the relevant condition
  filter(condition == "production") |> 
  select(item, starts_with("scores_produ")) |> 
  mutate(scores_production_distractor = 
           log(exp(scores_production_distractor1) + exp(scores_production_distractor2))) |> 
  select(-scores_production_distractor1, -scores_production_distractor2) |> 
  group_by(item) |> 
  mutate(n = n()) |>  
  pivot_longer(-c("item", "n")) |> 
  separate(name, into = c("Variable", "condition", "response"), sep = "_") |> 
  select(-Variable) |> 
  mutate(prob = exp(value)) |>
  # mutate(prob = value) |> 
  group_by(item, condition) |> 
  mutate(prob = prob / sum(prob) * n) |> 
  ungroup()

d_llm_prob_averages_prod <- d_llm_prob_prod |> 
  group_by(condition, response) |> 
  summarize(prob = mean(prob)) |>
  mutate(response = factor(response, levels = tcd)) |> 
  mutate(condition = 'production') |> 
  arrange(condition, response)
  
d_llm_prob_inter <- d |> 
  # use only predictions for items in the relevant condition
  filter(condition == "interpretation") |> 
  select(item, starts_with("scores_inter")) |> 
  group_by(item) |> 
  mutate(n = n()) |>  
  pivot_longer(-c("item", "n")) |> 
  separate(name, into = c("Variable", "condition", "response"), sep = "_") |> 
  select(-Variable) |> 
  mutate(prob = exp(value)) |>
  # mutate(prob = value) |> 
  group_by(item, condition) |> 
  mutate(prob = prob / sum(prob) * n) |> 
  ungroup() 

d_llm_prob_averages_inter <- d_llm_prob_inter |> 
  group_by(condition, response) |> 
  summarize(prob = mean(prob)) |> 
  mutate(response = factor(response, levels = tcd)) |> 
  mutate(condition = 'interpretation') |> 
  arrange(condition, response)

d_llm_prob_averages <- 
  rbind(d_llm_prob_averages_prod,
        d_llm_prob_averages_inter) |> 
  mutate(condition = factor(condition, levels = c('production','interpretation')))

# which model to test?
if (model == "probability") {
  prob_prod  <- matrix(d_llm_prob_averages_prod  |> pull(prob), nrow = 1)
  prob_inter <- matrix(d_llm_prob_averages_inter |> pull(prob), nrow = 1)
} else {
  prob_prod  <- matrix(LLM_pred_new_prod,  nrow = 1)  
  prob_inter <- matrix(LLM_pred_new_inter, nrow = 1)
}

d_llm_prob_averages <- 
  tibble(condition = factor(c(rep("production",3), rep("interpretation",3)),
                            levels = c("production", "interpretation")),
         response = rep(d_llm_prob_averages_prod$response,2),
         prob = c(prob_prod, prob_inter))

model_predictions_vanilla <- d_llm_prob_averages |> 
  ggplot(aes(x = response, y = prob, fill = response)) +
  geom_col(position = "dodge") +
  facet_grid(condition ~ .) +
  ylab("") + xlab("") +
  theme(legend.position="none") +
  ylim(c(0,1)) +
  ggtitle("model predictions") +
  theme(axis.text.x = element_text(angle = 25, vjust = 1, hjust=1)) +
  theme(strip.text = element_text(size = 12)) + 
  theme(plot.title = element_text(size = 12))

model_predictions_vanilla


ggsave(plot = model_predictions_vanilla, 
       filename = "../04-paper/00-pics/model-predictions-vanilla.pdf", 
       height = 4, width = 3, scale = 1.1)

```

## Adding a link function:  the $\alpha$-model

On the surface, the LLM predictions show a similar ordinal pattern as the human data.
The target option is preferred over the competitor, which is in turn preferred over the distractor option(s).
Moreover, the amount of target choices is higher in the production condition than in the interpretation condition.
However, the LLM predictions have lower entropy than the distribution of the human answers.
The LLM prediction could be said to be more "optimized", having far fewer distractor choices and a higher ratio of target choices than the human data.

But this does not mean that we should reject the idea that the LLM predictions could serve as a model of human choice data quite yet.
It depends on what we take the model's prediction to be.
If we take the model's prediction to be the precise unalterable probabilities shown in Figure <span style = "color:firebrick">XXX (vanilla predictions)</span>, the match to human answers is not great.
But we may be more lenient and modulate the model's predictions with an additional link function, thereby alterating -as it were- the entropy in the predicted probability vector.

One way of doing this is to use a power-law link function with variable parameter $\alpha$:

$$\text{Prediction}(o_i, \alpha) = \frac{P(o_i)^\alpha}{\sum_{o_j} P(o_j)^\alpha}$$

The previous vanilla predictions in Equation <span style = "color:firebrick">XXX</span> are a special case, assuming a fixed $\alpha =1$.
Notice that this is equivalent to a soft-max linking function on the underlying log-probabilities returned by the LLM.

We use Bayesian inference to obtain posterior estimates of credible values of $\alpha$, once for the production data and the model's predictions for the production condition, and once for the interpretation side.
In both cases, the model uses a single $\alpha$ with a log-normal prior, $\alpha \sim \text{log-Normal}(0.5,1)$ and the obvious multinomial likelihood function.
The usual Bayesian point and interval summary statistics for the two inferences are:


```{r stan-fit-average-predictions}
#| message: false
#| warning: false
#| error: false
#| output: false

# production
d_production <- d |> 
  filter(condition == "production") |> 
  mutate(response = case_when(response == "distractor1" ~ "distractor",
                              response == "distractor2" ~ "distractor",
                              TRUE ~ response)) |> 
  pull(response) |> 
  match(c("target", "competitor", "distractor"))

prob_production <- d_llm_prob_averages |> 
  filter(condition == "production") |> 
  pull(prob)

d_prod    <- matrix(table(d_production) |> as.integer(), nrow = 1)

fit_production   <- fit_data(d_prod, prob_prod) 
alpha_production <- fit_production$draws(format = "df") |> pull(alpha)

# interpretation
d_interpretation <- d |> 
  filter(condition == "interpretation") |> 
  pull(response) |> 
  match(c("target", "competitor", "distractor"))

prob_interpretation <- d_llm_prob_averages |> 
  filter(condition == "interpretation") |> 
  pull(prob)

d_inter    <- matrix(table(d_interpretation) |> as.integer(), nrow = 1)

fit_interpretation <- fit_data(d_inter, prob_inter) 
alpha_interpretation <- fit_interpretation$draws(format = "df") |> pull(alpha) 

```

```{r show-summary-stats}
rbind(
  aida::summarize_sample_vector(alpha_production, name = "production"),
  aida::summarize_sample_vector(alpha_interpretation, name = "interpretation"),
  aida::summarize_sample_vector(alpha_production - alpha_interpretation, name = "diff. prod-inter")
)
```

We see that only values for $\alpha$ below 1 are credible, which is in line with the previous observation that the human data distribution is more entropic than the vanilla model predictions.
The table also shows posterior summary statistics for the difference vector of $\alpha$ values for the production case minus those for the interpretation case.
These differences are not credibly different from zero, suggesting that the the same $\alpha$ parameter might be used for both production and interpretation cases (at least for the data at hand).

Figure <span style = "color:firebrick">XXX</span> shows the posterior predictive distributions from the two model fits.
The colored bars show the observed counts.
The black dots are the means of the posterior predictives and the black bars given the 95% HDI of the posterior predictive distribution.
The data observations lie inside the posterior predictive 95% HDI for the interpretation condition, suggesting that the fitted model would predict data that looks similar to what the model was trained on.
For the production data, on the other hand, this is not the case.
Even with the ability to fit $\alpha$ to the data, the model is not able to reproduce the observed pattern in the data.



```{r get-posterior-predictives-alpha-model}

post_pred_prod <- get_posterior_predictives(fit_production, d_prod, "post_pred_prod") |> 
  mutate(condition = "production")
post_pred_inter <- get_posterior_predictives(fit_interpretation, d_inter, "post_pred_inter") |> 
  mutate(condition = "interpretation")

post_pred <- rbind(post_pred_inter, post_pred_prod) |> 
  mutate(condition = factor(condition, levels = c("production", "interpretation"))) 

post_pred |> 
  ggplot(aes(x = response, y = observed, fill = response)) +
  geom_col() +
  facet_grid(.~condition) +
  geom_pointrange(aes(x = response, y = mean, ymin = `|95%`, ymax = `95%|`), 
                  size = 0.7, linewidth = 1) +
  ylab("") + xlab("") +
  theme(legend.position="none") +
  theme(axis.text.x = element_text(angle = 25, vjust = 1, hjust=1)) +
  theme(strip.text = element_text(size = 12))

ggsave(filename = "../04-paper/00-pics/PPC-alpha-model.pdf", width = 8, height = 3.5, scale = 0.9)
  

```

The interpreation of the visual PPC is corrobarete by a Bayesian posterior predictive check:

```{r Bayesian-p}
message("Bayesian p value for production: ", extract_bayesian_p(fit_production))
message("Bayesian p value for interpretation: ", extract_bayesian_p(fit_interpretation))
```


Intuitively, there are two reasons for the model's failure on production data.
First, since the model predicts a very low rate of distractor choices, we would need a rather low $\alpha$ to increas this probability.
But, second, for low-ish values of $\alpha$, the model cannot predict the large probability ratio in favor of target choice over competitor choices.
In sum, the model is unable to capture the quantitive patter (i.e., the set of probability ratios among choice options) adequately for the production case.



<span style = "color:firebrick">could include:</span>

- doing soft-max on probabilities (not log-probs) is a far worse model; maybe mention?

## Adding noise: the $\alpha \epsilon$-model

The LLM's failure to predict the human production is due, at least in part, to under-predicting the rate at which human participants selected the distractor.
But a human selection of a distractor is, most naturally, explained as an error anyway.
In experimental modeling work, it is therefore common to equip the predictive models also with a low error-probability, e.g., an $\epsilon$ chance of selecting a random option.

We therefore fit another model with two parameters: $\alpha$ as before and $\epsilon$ with prior $\epsilon \sim \text{Exponential}(1)$ and define the multinomial choice probability for option $o_i$ as:

$$\text{Prediction}(o_i, \alpha, \epsilon) = \frac{P(o_i)^\alpha + \epsilon}{\sum_{o_j} \left( P(o_j)^\alpha + \epsilon \right)}$$

Summary statistics for the Bayesian posterior are shown in the table below.
We see that $\alpha$ now has higher estimates for the production model, because $\epsilon$-errors can help explain the distractor choices.
The $\alpha$ estimated for the production data is now credibly higher than for the interpretation data, which makes intuitive sense, given that the prduction condition is inuitively easier.
There is no credible difference in estimated $\epsilon$ between production and interpretation model.

```{r}
#| output: false

fit_production_eps <- fit_data(d_prod, prob_prod, 
                           model_name = '../01-code/llm-average-matrix-epsilon-arrayed.stan')
fit_interpretation_eps <- fit_data(d_inter, prob_inter,
                               model_name = '../01-code/llm-average-matrix-epsilon.stan')
```

```{r}

produce_summary_prodInt_epsilonAlpha(fit_production_eps, fit_interpretation_eps)

```

Figure <span style = "color:firebrick">XXX</span> shows a visual posterior predictive check for the $\alpha\epsilon$-model.
When accommodating for an $\epsilon$ chance of random error, the LLM-based models do not fail the visual check: once trained on the data, they are no longer surprised by it.
This is a low bar to pass, but it is nonetheless reassuring that probabilistic predictions obtained from an LLM can serve as input to a probabilistic model predicting quantitative pattersn in human choice data.

```{r}
post_pred_prod_eps <- get_posterior_predictives(fit_production_eps, d_prod, 
                                                "post_pred_prod_eps") |> 
  mutate(condition = "production")
post_pred_inter_eps <- get_posterior_predictives(fit_interpretation_eps, d_inter,
                                                 "post_pred_inter_eps") |> 
  mutate(condition = "interpretation")

post_pred_eps <- rbind(post_pred_inter_eps, post_pred_prod_eps) |> 
  mutate(condition = factor(condition, levels = c("production", "interpretation"))) 

post_pred_eps |> 
  ggplot(aes(x = response, y = observed, fill = response)) +
  geom_col() +
  facet_grid(.~condition) +
  geom_pointrange(aes(x = response, y = mean, ymin = `|95%`, ymax = `95%|`), size = 0.7, linewidth = 1) +
  ylab("") + xlab("") +
  theme(legend.position="none") +
  theme(axis.text.x = element_text(angle = 25, vjust = 1, hjust=1)) +
  theme(strip.text = element_text(size = 12))

ggsave(filename = "../04-paper/00-pics/PPC-alpha-eps-model.pdf", width = 8, height = 3.5, scale = 0.9)

```

We can corroborate this intuition with a Bayesian posterior $p$-value for both model fits:

```{r Bayesian-p-alpha-epsilon}
message("Bayesian p value for production:", extract_bayesian_p(fit_production_eps))
message("Bayesian p value for interpretation:", extract_bayesian_p(fit_interpretation_eps))
```


### Sanity check: Range of prediction of the $\alpha\epsilon$-model

see file `05-prediction-range.R`

# Individual-item analysis

## Look at arg-max model predictions

Human choice frequencies for when arg-max model would predict target or non-target; indeed slightly different outcomes also for humans.

```{r}
d |> 
  mutate(response_binary = ifelse(response == "target", "target", "other")) |> 
  count(condition, prediction, response) |>  
  group_by(condition, prediction) |> 
  mutate(total = sum(n),
         proportion = n / total) |> 
  # pivot_wider(id_cols = 1:2, names_from = response, values_from = proportion) |> 
  ggplot(aes(x = response, y = proportion, fill = response)) + 
  facet_grid(condition ~ prediction) +
  geom_col()
```

## Fit an $\alpha \epsilon$ model to the by-item aggregate data

Check if an $\alpha \epsilon$ model predicts correctly when it has to predict the data from each item individually.

First get the predictions for each each item:

```{r extract-byItem-LLM-predictions}
d_item_analysis <- d_llm_prob_prod |>
  rbind(d_llm_prob_inter) |>
  mutate(condition = factor(condition, levels = c('production', 'interpretation'))) |>
  full_join(d |>
              select(item, feature_set, position_production, position_interpretation) |>
              unique(),
            by = 'item') |>
  mutate(position = case_when(condition == "production" ~ position_production,
                              TRUE ~ position_interpretation)) |>
  # filter(response != "distractor") |>
  unique() |>
  pivot_wider(id_cols = c(item, condition, feature_set, position_production, position_interpretation, position),
              names_from = response, values_from = prob) |> 
  arrange(condition, item)

```

Prepare by-item data to fit, starting with production.

```{r prepare-data-for-byItem-fitting-production}

# production

d_counts_items <- d |> 
  count(condition, item, response) |> 
  group_by(condition, item) |> 
  mutate(total = sum(n)) |> 
  ungroup() |> 
  mutate(proportion = n / total) |> 
  arrange(condition, item, response)

d_counts_items_matrix_prod <- d_counts_items |> 
  filter(condition == "production") |> 
  pivot_wider(id_cols = item, names_from = response, values_from = n, values_fill = 0) |> 
  select(-item) |> 
  as.matrix()

prob_items_prod <- d_item_analysis |>
  filter(condition == "production") |> 
  group_by(item) |> 
  summarize(
    target = mean(target),
    competitor = mean(competitor),
    distractor = mean(distractor)
    ) |> 
  ungroup() |> 
  select(-item) |> 
  as.matrix()

```

And for interpretation.

```{r prepare-data-for-byItem-fitting-interpretation}

# interpretation

d_counts_items_matrix_inter <- d_counts_items |> 
  filter(condition == "interpretation") |> 
  pivot_wider(id_cols = item, names_from = response, values_from = n, values_fill = 0) |> 
  select(-item) |> 
  as.matrix()

prob_items_inter <- d_item_analysis |>
  filter(condition == "interpretation") |> 
  group_by(item) |> 
  summarize(
    target = mean(target),
    competitor = mean(competitor),
    distractor = mean(distractor)
    ) |> 
  ungroup() |> 
  select(-item) |> 
  as.matrix()

```


```{r model-fit-byItem}
#| message: false
#| warning: false
#| error: false
#| output: false

fit_items_prod <- fit_data(
  d_counts_items_matrix_prod, 
  prob_items_prod, 
  model_name = '../01-code/llm-average-matrix-epsilon.stan')

fit_items_inter <- fit_data(
  d_counts_items_matrix_inter, 
  prob_items_inter, 
  model_name = '../01-code/llm-average-matrix-epsilon.stan')
```

The summary stats for the feature-wise model are:
( <span style = "color:firebrick"> TODO: interpret this </span>)

```{r stats-for-fits-feature-sets}
produce_summary_prodInt_epsilonAlpha(fit_items_prod, fit_items_inter)
```

The visual posterior predictive fit for the feature-wise $\alpha\epsilon$-model is:

```{r PPC-items}

# rerun = FALSE

# production
post_pred_items_prod <- 
  get_posterior_predictives(
    fit_items_prod,
    d_counts_items_matrix_prod,
    "post-pred-prod-item"
    ) |> 
  group_by(row) |> 
  mutate(total = sum(observed)) |> 
  filter(response == "target") |> 
  mutate(row = factor(row))

post_pred_items_prod |> 
  ggplot(aes(x = fct_reorder(row, mean/total), y = observed/total)) +
  geom_pointrange(aes(y = mean/total, ymin = `|95%`/total, ymax = `95%|`/total), 
                  size = 0.6, linewidth = 1, color = "lightgray") +
  geom_point(aes(y = mean/total), color = "darkgray") +
  geom_point(color = project_colors[2]) +
  coord_flip() +
  xlab("item") +
  ylab("proportion of target") +
  theme(axis.text.y = element_blank(),
           axis.ticks.y = element_blank()) +
  ggtitle("production")

ggsave(filename = "~/Desktop/post-pred-production.png", scale = 0.6)

post_pred_items_prod |> 
  ggplot(aes(x = mean/total, observed / total)) +
  geom_segment((aes(x = 0, y = 0, xend = 1, yend=1)), color = "gray") +
  geom_point(alpha = 0.8) +
  xlim(c(0,1)) +
  ylim(c(0,1)) + 
  ylab("observed") +
  xlab("predicted") +
  ggtitle("production")

ggsave(filename = "~/Desktop/obs-pred-production.png", scale = 0.6)

  
# interpretation
post_pred_items_inter <- 
  get_posterior_predictives(
    fit_items_inter,
    d_counts_items_matrix_inter,
    "post-pred-inter-item"
    ) |> 
  group_by(row) |> 
  mutate(total = sum(observed)) |> 
  filter(response == "target") |> 
  mutate(row = factor(row))

post_pred_items_inter |> 
  ggplot(aes(x = fct_reorder(row, mean/total), y = observed/total)) +
  geom_pointrange(aes(y = mean/total, ymin = `|95%`/total, ymax = `95%|`/total), 
                  size = 0.6, linewidth = 1, color = "lightgray") +
  geom_point(aes(y = mean/total), color = "darkgray") +
  geom_point(color = project_colors[2]) +
  coord_flip() +
  xlab("item") +
  ylab("proportion of target") +
  theme(axis.text.y = element_blank(),
           axis.ticks.y = element_blank()) +
  ggtitle("interpretation")

ggsave(filename = "~/Desktop/post-pred-interpretation.png", scale = 0.6)

post_pred_items_inter |> 
  ggplot(aes(x = mean/total, y =  observed/ total)) +
  # geom_segment(aes(x = `|95%`/total, xend = `95%|`/total, yend = observed/total), color = "gray", alpha = 0.7) +
  geom_segment((aes(x = 0, y = 0, xend = 1, yend=1)), color = "gray") +
  geom_point(alpha = 0.8) +
  xlim(c(0,1)) +
  ylim(c(0,1)) + 
  ylab("observed") +
  xlab("predicted") +
  ggtitle("interpretation")

ggsave(filename = "~/Desktop/obs-pred-interpretation.png", scale = 0.6)
```


Bayesian $p$-values for these by-item level fits:

```{r Bayesian-p-alpha-epsilon-byItem}
message("Bayesian p value for production:", extract_bayesian_p(fit_items_prod))
message("Bayesian p value for interpretation:", extract_bayesian_p(fit_items_inter))
```












# Beyond grand aggregation

The LLM-based $\alpha \epsilon$-model makes different predictions for different prompts.
The following plot shows the vanilla predictions for the target choice option for different `feature sets`.
A feature set is a pair consisting of the trigger feature and the nuisance feature. 
The `trigger feature` is the feature (color, shape or texture) that should be selected in the production condition and that the trigger word in the interpretation condition is an instance of. <span style = "color:firebrick"> TODO: give example</span>.
The `nuisance feature` is the feature whose level all objects share, so that it can, strictly speaking, be completely ignored.
The figure below shows that, when `color` is the trigger feature and `texture` is the nuisance feature, the models' predictions are consistently close to one in the production condition.
But for the feature set `shape-texture`, the model may assign an almost zero probability to the target choice.


```{r}

d_item_analysis |>
  rename(`feature set` = feature_set) |> 
  ggplot(aes(x = `feature set`, y = target, color = `feature set`, shape = `feature set`)) +
  geom_jitter(width = 0.3, size = 1.8) + 
  facet_grid(. ~ condition) +
  xlab("") +
  ylab("predicted target probability") +
  theme(axis.text.x = element_text(angle = 25, vjust = 1, hjust=1)) +
  theme(axis.title.y = element_text(size = 14)) +
  theme(strip.text = element_text(size = 12)) + 
  theme(plot.title = element_text(size = 12)) +
  theme(legend.position="none")

ggsave(filename = "../04-paper/00-pics/feature-set-variation.pdf",
       width = 8, height = 4, scale = 0.95)

```

Likewise, the LLM's predictions seem to depend on the order of the choice options.
This effect is less pronounced for the production condition, as shown in the following figure (where a sequence like `[t,d,c,d]` represents a context in which the target option was presented first, then the first distractor, than the competitor and finally the second distractor).

```{r}
d_item_analysis |>
  filter(condition == "production") |> 
  # mutate(position = droplevels(position)) |> 
  ggplot(aes(x = position_production, y = target, color = position)) +
  geom_jitter(width = 0.3) + 
  ylab("predicted target probability") +
  theme(axis.text.x = element_text(angle = 25, vjust = 1, hjust=1)) +
  theme(axis.title.y = element_text(size = 14)) +
  theme(strip.text = element_text(size = 12)) + 
  theme(plot.title = element_text(size = 12)) +
  theme(legend.position="none") +
  xlab("order of choice options") +
  ggtitle("production")

ggsave(filename = "../04-paper/00-pics/order-variation-production.pdf",
       width = 16*0.575, height = 4, scale = 0.7)
```

The effect is more pronounced for the interpretation condition.
For example instances of the ordering `[c,t,d]` are assigned very low target probabilities.

```{r}
d_item_analysis |>
  filter(condition == "interpretation") |> 
  ggplot(aes(x = position, y = target, color = position, shape = position)) +
  geom_jitter(width = 0.3) + 
  ylab("predicted target probability") +
  xlab("") +
  theme(axis.text.x = element_text(angle = 25, vjust = 1, hjust=1)) +
  theme(axis.title.y = element_text(size = 14)) +
  theme(strip.text = element_text(size = 12)) + 
  theme(plot.title = element_text(size = 12)) +
  theme(legend.position="none") +
  xlab("order of choice options") +
  ggtitle("interpretation")

ggsave(filename = "../04-paper/00-pics/order-variation-interpretation.pdf",
       width = 16*0.375, height = 4, scale = 0.7)
```

This demonstrates that the model's predictions are quite sensitive to differences in the context, such as the kinds of features that matter for the language task, or the order of presentation of the choice options.
The questions to be asked is therefore: do we see similar patterns also in the human data?; or should we conceive of LLMs as predictors only after aggregating over "spurious contextual elements" like order of presentation of alternatives?

## Effects of relevant features

We first investigate whether the features relevant for the communicative task have an influence on human choice behavior, and whether the LLM predictions specific to each level of `feature set` yield adequate predictions for the human data.

### Human data

First, here are the choice counts for the human data broken up by `feature set`, starting with the production condition:

```{r human-data-counts-per-featureSet}
d_counts_features <- d |> 
  count(condition, feature_set, response) |> 
  group_by(condition, feature_set) |> 
  mutate(total = sum(n)) |> 
  ungroup() |> 
  mutate(proportion = n / total) |> 
  arrange(condition, feature_set, response)

d_counts_features |> 
  filter(condition == "production") |> 
  ggplot(aes(x = response, y = n/total, fill = response)) +
  geom_col(position = "dodge") +
  # facet_grid(condition ~ feature_set) +
  ylab("proportion") + xlab("") +
  facet_wrap(. ~ feature_set, ncol = 3) +
  theme(legend.position="none") +
  ggtitle("production") +
  theme(axis.text.x = element_text(angle = 25, vjust = 1, hjust=1))
```

There are some slight numerical differences, but maybe not too noteworthy.
To test whether differences for different features matter to human answer behavior in the production task, we ran a Bayesian logistic regression and checked whether the estimated proportions for conditions with, respectively, the highest (`shape-color`) and lowest observed target choice proportion (`shape-texture`) are credibly different.
We find no credible difference.
This suggests that human data has no noteworthy systematic variation based on which features are relevant for the production task.

```{r BDA-features-production}
#| output: false

# shape-color mas max. target proportion
# shape-texture is min target proportion

fit_features_prod <- 
  brms::brm(response ~ feature_set,
    data = d_counts_features |> 
      filter(condition == "interpretation") |> 
      uncount(n) |> 
      mutate(response = case_when(response == "distractor" ~ "competitor",
                                  TRUE ~ response)),
    family = bernoulli()
  )

```

```{r BDA-compare-prod}
faintr::compare_groups(
  fit_features_prod,
  higher = feature_set == "shape-color",
  lower  = feature_set == "shape-texture"
)
```

The picture for the interpretation task looks different.
There are conditions where the target is clearly chosen more frequently (`shape-color`), but others where the modal choice is the distractor (`color-texture`).

```{r human-data-counts-per-featureSet-interpretation}

d_counts_features |> 
  filter(condition == "interpretation") |> 
  ggplot(aes(x = response, y = n/total, fill = response)) +
  geom_col(position = "dodge") +
  # facet_grid(condition ~ feature_set) +
  ylab("proportion") + xlab("") +
  facet_wrap(. ~ feature_set, ncol = 3) +
  theme(legend.position="none") +
  ggtitle("interpretation") +
  theme(axis.text.x = element_text(angle = 25, vjust = 1, hjust=1))

```

To test whether these differences are noteworthy, we ran another a Bayesian logistic regression comparing the estimated proportions for conditions with, respectively, the highest (`shape-color`) and lowest observed target choice proportion (`color-texture`).
Indeed, it turns out that they are credibly different.

```{r BDA-features-interpretation}
#| output: false

# max target prop: shape-color
# min target prop: color-texture

fit_features_inter <- 
  brms::brm(response ~ feature_set,
    data = d_counts_features |> 
      filter(condition == "interpretation") |> 
      uncount(n) |> 
      mutate(response = case_when(response == "distractor" ~ "competitor",
                                  TRUE ~ response)),
    family = bernoulli()
  )


```


```{r BDA-features-compare-groups-interpretations}
faintr::compare_groups(
  fit_features_inter,
  higher = feature_set == "shape-color",
  lower  = feature_set == "color-texture"
)
```

Using leave-one-out model comparison to pin an intercept-only model against the the previous model with a single predictor `feature_set`, we find that the intercept-only model is numerically better, but not noteworthily so.

```{r BDA-features-LOO-inter}
#| message: false
#| warning: false
#| error: false
#| output: false

fit_features_inter_Intercept <- 
  brms::brm(response ~ 1,
    data = d_counts_features |> 
      filter(condition == "interpretation") |> 
      uncount(n) |> 
      mutate(response = case_when(response == "distractor" ~ "competitor",
                                  TRUE ~ response)),
    family = bernoulli()
  )

```

```{r BDA-features-LOO-inter-results}
loo_compare(fit_features_inter |> loo(), fit_features_inter_Intercept |> loo())
```

In sum, while there is not indication that human production data depends on the kind of task-relevant features, the human interpration data may be susceptible to variation in task-relevant features.

### Model predictions

The model's predictions do seem to vary based on task-relevant features, at least for the interpretation condition.
We therefore fit another $\alpha\epsilon$-model which differs from the previous in that it does not aim to explain the grand-average of choice proportions, but the choice proportions for each constellation of task-relevant features.
(For clarity: we use a single parameter pair $\alpha$ and $\epsilon$, but use it to predict count data for six different conditions based on six potentially different predictions made by the LLM.)

```{r}
d_counts_features_matrix_prod <- d_counts_features |> 
  filter(condition == "production") |> 
  pivot_wider(id_cols = feature_set, names_from = response, values_from = n) |> 
  select(-feature_set) |> 
  as.matrix()
dimnames(d_counts_features_matrix_prod)[[1]] <- 
  d_counts_features |> 
  pull(feature_set) |> unique()

d_counts_features_matrix_inter <- d_counts_features |> 
  filter(condition == "interpretation") |> 
  pivot_wider(id_cols = feature_set, names_from = response, values_from = n) |> 
  select(-feature_set) |> 
  as.matrix()
dimnames(d_counts_features_matrix_inter)[[1]] <- 
  d_counts_features |> 
  pull(feature_set) |> unique()

prob_features_prod <- d_item_analysis |>
  filter(condition == "production") |> 
  group_by(feature_set) |> 
  summarize(
    target = mean(target),
    competitor = mean(competitor),
    distractor = mean(distractor)
    ) |> 
  ungroup() |> 
  select(-feature_set) |> 
  as.matrix()

prob_features_inter <- d_item_analysis |>
  filter(condition == "interpretation") |> 
  group_by(feature_set) |> 
  summarize(
    target = mean(target),
    competitor = mean(competitor),
    distractor = mean(distractor)
    ) |> 
  ungroup() |> 
  select(-feature_set) |> 
  as.matrix()

```

```{r fit-alpha-epsilon-model-feature-sets}
#| message: false
#| warning: false
#| error: false
#| output: false

fit_features_prod <- fit_data(
  d_counts_features_matrix_prod, 
  prob_features_prod, 
  model_name = '../01-code/llm-average-matrix-epsilon.stan')

fit_features_inter <- fit_data(
  d_counts_features_matrix_inter, 
  prob_features_inter, 
  model_name = '../01-code/llm-average-matrix-epsilon.stan')
```

The summary stats for the feature-wise model are:
( <span style = "color:firebrick"> TODO: interpret this </span>)

```{r stats-for-fits-feature-sets}
produce_summary_prodInt_epsilonAlpha(fit_features_prod, fit_features_inter)
```

The visual posterior predictive fit for the feature-wise $\alpha\epsilon$-model is:

```{r PPC-features}
# production
post_pred_features_prod <- 
  get_posterior_predictives(
    fit_features_prod,
    d_counts_features_matrix_prod
    )

post_pred_features_prod |> 
  ggplot(aes(x = response, y = observed, fill = response)) +
  geom_col() +
  facet_wrap(.~ rownames, ncol=3) +
  geom_pointrange(aes(x = response, y = mean, ymin = `|95%`, ymax = `95%|`), 
                  size = 0.7, linewidth = 1) +
  ylab("") +
  theme(legend.position="none") +
  ggtitle("production") +
  theme(axis.text.x = element_text(angle = 25, vjust = 1, hjust=1))


# interpretation
post_pred_features_inter <- 
  get_posterior_predictives(
    fit_features_inter,
    d_counts_features_matrix_inter
    )

post_pred_features_inter |> 
  ggplot(aes(x = response, y = observed, fill = response)) +
  geom_col() +
  facet_wrap(.~ rownames, ncol=3) +
  geom_pointrange(aes(x = response, y = mean, ymin = `|95%`, ymax = `95%|`), 
                  size = 0.7, linewidth = 1) +
  ylab("") +
  theme(legend.position="none") +
  ggtitle("interpretation") +
  theme(axis.text.x = element_text(angle = 25, vjust = 1, hjust=1))

```

For the production data, where there was little ground for suspecting variation in the human data, the model's feature-wise prediction are reasonable, with the sole expection of perhaps the `shape-texture` condition.
But for the interpretation data, there is mild reason for diagnosing a potential systematic difference: in conditions where the human data suggests that target and competitor choices are roughly equal (`color-texture` and `texture-shape`), the model still predicts that target choices should be preferred; reversely, where the model predicts that target and distractor choices are rather similar, the `texture-color` condition, the human data is biased towards the target.
( <span style = "color:firebrick">TODO: rethink conclusion</span>)


## Effects of order of choice options

### Human data

Human choice counts for each order of answer alternatives are shown below.

- for production: there is some variation but the target is consistently the preferred option (by a margin)
- for interpretation: there are cases where the competitor is chose more frequently than the target

```{r human-data-counts-per-choiceOrder}

d_counts_choice <- d |> 
  count(condition, position, response) |> 
  group_by(condition, position) |> 
  mutate(total = sum(n)) |> 
  ungroup() |> 
  mutate(proportion = n / total) |> 
  arrange(condition, position, response)

d_counts_choice |> 
  filter(condition == "production") |> 
  ggplot(aes(x = response, y = n, fill = response)) +
  geom_col(position = "dodge") +
  facet_wrap(. ~ position, ncol = 4) +
  ylab("") + xlab("") +
  theme(legend.position="none") +
  ggtitle("production") +
  theme(axis.text.x = element_text(angle = 25, vjust = 1, hjust=1)) +
  coord_flip()
  # ggtitle("production")

d_counts_choice |> 
  filter(condition == "interpretation") |> 
  ggplot(aes(x = response, y = n, fill = response)) +
  geom_col(position = "dodge") +
  facet_wrap(. ~ position, ncol = 3) +
  ylab("") + xlab("") +
  theme(legend.position="none") +
  ggtitle("interpretation") +
  theme(axis.text.x = element_text(angle = 25, vjust = 1, hjust=1))
  # coord_flip() + 
  # ggtitle("interpretation")
```

BDA logistic comparing numerically highest and lowest options ... not credibly different for production.

```{r BDA-choices-production}
#| output: false

d_counts_choice |>
  filter(condition == "production", response == "target") |>
  arrange(proportion)

# max target proportion: [d,t,c,d]
# min target proportion: [d,t,d,c]

fit_choice_prod <- 
  brms::brm(response ~ position,
    data = d_counts_choice |> 
      filter(condition == "production") |> 
      uncount(n) |> 
      mutate(response = case_when(response == "distractor" ~ "competitor",
                                  TRUE ~ response)),
    family = bernoulli()
  )

```

```{r BDA-choices-production-results}
faintr::compare_groups(
  fit_choice_prod,
  higher = position == "[d,t,c,d]",
  lower  = position == "[d,t,d,c]"
)

```

... but for interpretation ...

```{r BDA-choices-interpretation}
#| output: false

# d_counts_choice |>
#   filter(condition == "interpretation", response == "target") |>
#   arrange(proportion)

# max target proportion: [d,t,c]
# min target proportion: [d,c,t]

fit_choice_inter <- 
  brms::brm(response ~ position,
    data = d_counts_choice |> 
      filter(condition == "interpretation") |> 
      uncount(n) |> 
      mutate(response = case_when(response == "distractor" ~ "competitor",
                                  TRUE ~ response)),
    family = bernoulli()
  )

```

```{r BDA-choices-interpretation-results}
faintr::compare_groups(
  fit_choice_inter,
  higher = position == "[d,t,c]",
  lower  = position == "[d,c,t]"
)
```

Loo-based model comparison for interpretation against intercept-only model shows that a model which includes order of choice options as an explanatory factor is significantly better than an intercept-only model.

```{r BDA-choice-LOO-inter}
#| message: false
#| warning: false
#| error: false
#| output: false

fit_choice_inter_Intercept <- 
  brms::brm(response ~ 1,
    data = d_counts_choice |> 
      filter(condition == "interpretation") |> 
      uncount(n) |> 
      mutate(response = case_when(response == "distractor" ~ "competitor",
                                  TRUE ~ response)),
    family = bernoulli()
  )


```

```{r BDA-choice-LOO-inter-results}
loo_compare(fit_choice_inter |> loo(), fit_choice_inter_Intercept |> loo())
```

We may conclude from this that there is reaonable support for the supposition that the human interpretation data is influenced by the order of presentation of choice alternatives.

### Model predictions

It remains to inspect whether the model, which also seems susceptible to choice-order, is compatible with the human data on a by choice-order level.

```{r extract-by-choiceOrder-counts}
d_counts_choice_matrix_prod <- d_counts_choice |> 
  filter(condition == "production") |> 
  pivot_wider(id_cols = position, names_from = response, values_from = n) |> 
  select(-position) |> 
  as.matrix()
d_counts_choice_matrix_prod[is.na(d_counts_choice_matrix_prod)] <- 0
dimnames(d_counts_choice_matrix_prod)[[1]] <- 
  d_counts_choice |> 
  filter(condition == "production") |> 
  pull(position) |> unique()

d_counts_choice_matrix_inter <- d_counts_choice |> 
  filter(condition == "interpretation") |> 
  pivot_wider(id_cols = position, names_from = response, values_from = n) |> 
  select(-position) |> 
  as.matrix()
dimnames(d_counts_choice_matrix_inter)[[1]] <- 
  d_counts_choice |> 
  filter(condition == "interpretation") |> 
  pull(position) |> unique()

prob_choice_prod <- d_item_analysis |>
  filter(condition == "production") |> 
  group_by(position) |> 
  summarize(
    target = mean(target),
    competitor = mean(competitor),
    distractor = mean(distractor)
    ) |> 
  ungroup() |> 
  select(-position) |> 
  as.matrix()

prob_choice_inter <- d_item_analysis |>
  filter(condition == "interpretation") |> 
  group_by(position) |> 
  summarize(
    target = mean(target),
    competitor = mean(competitor),
    distractor = mean(distractor)
    ) |> 
  ungroup() |> 
  select(-position) |> 
  as.matrix()

```

```{r fit-alpha-epsilon-model-choice}
#| message: false
#| warning: false
#| error: false
#| output: false

fit_choice_prod <- fit_data(
  d_counts_choice_matrix_prod, 
  prob_choice_prod, 
  model_name = '../01-code/llm-average-matrix-epsilon.stan')

fit_choice_inter <- fit_data(
  d_counts_choice_matrix_inter, 
  prob_choice_inter, 
  model_name = '../01-code/llm-average-matrix-epsilon.stan')
```

The summary stats for the feature-wise model are:
( <span style = "color:firebrick"> TODO: interpret this </span>)

```{r stats-for-fits-choice}
produce_summary_prodInt_epsilonAlpha(fit_choice_prod, fit_choice_inter)
```

The visual posterior predictive fits for the feature-wise $\alpha\epsilon$-model for the production model is:

```{r PPC-choice}
# production
post_pred_choice_prod <- 
  get_posterior_predictives(
    fit_choice_prod,
    d_counts_choice_matrix_prod
    )

post_pred_choice_prod |> 
  ggplot(aes(x = response, y = observed, fill = response)) +
  geom_col() +
  facet_wrap(.~ rownames, ncol=3) +
  geom_pointrange(aes(x = response, y = mean, ymin = `|95%`, ymax = `95%|`), 
                  size = 0.7, linewidth = 1) +
  ylab("") +
  theme(legend.position="none") +
  ggtitle("production") +
  theme(axis.text.x = element_text(angle = 25, vjust = 1, hjust=1))
```

The posterior predictives fo the LLM-based model are decent, except for one case: in order condition `[t,d,d,c]` the model predicts a higher competitor choice probability, but this pattern is not supported by the human data.


The PPC for the interpretation data is: 

```{r PPC-choice-interpretation}
# interpretation
post_pred_choice_inter <- 
  get_posterior_predictives(
    fit_choice_inter,
    d_counts_choice_matrix_inter
    )

post_pred_choice_inter |> 
  ggplot(aes(x = response, y = observed, fill = response)) +
  geom_col() +
  facet_wrap(.~ rownames, ncol=3) +
  geom_pointrange(aes(x = response, y = mean, ymin = `|95%`, ymax = `95%|`), 
                  size = 0.7, linewidth = 1) +
  ylab("") +
  theme(legend.position="none") +
  ggtitle("interpretation") +
  theme(axis.text.x = element_text(angle = 25, vjust = 1, hjust=1))

```

Here, the model's predictions are strictly speaking off in a number of cases (e.g., severely overpredicting the distractor choice rate), but they are qualitatively quire remarkable.
In cases where the human data favors the competitor (top row), the model also predicts relatively higher choice rates for the competitor, and vice versa.

# Conclusion

It is not entirely ludicrous to use numerical predictions from LLMs are part of predictive probabilistic models.
But we have to be careful.
Ideally, we average out over low-level variation, such as from order of presentation or similar "nuisance," at least as long as we do not understand what causes this variation in the predictions of models and further research that investigates when exactly this variation accords with empirically observed patterns.

This also implies that we should likely *not* (yet) aspire to use LLMs are models or individual- or item-level predictors.

( <span style = "color:firebrick">TODO: rethink conclusion</span>)


<span style = "color:firebrick">to be continued</span>

<!-- - there is not enough data to investigate `target ~ feature_set * position` -->

<!-- ```{r} -->
<!-- x <- d_item_analysis |> -->
<!--   filter( condition == "production" ) |> -->
<!--   group_by(feature_set, position_production) |> -->
<!--   tidyboot_mean(target) -->

<!-- x |> -->
<!--   ggplot(aes(x = feature_set, y = mean, fill = feature_set)) + -->
<!--   geom_col() + -->
<!--   geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper)) + -->
<!--   ggplot2::coord_flip() + -->
<!--   facet_wrap(~position_production) -->

<!-- ``` -->

























